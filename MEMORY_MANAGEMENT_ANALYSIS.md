# Memory Management and Crash Handling Analysis
## Brain in a Jar - Jetson Orin Deployment

**Analysis Date**: 2025-11-16  
**System**: Jetson Orin AGX (32GB RAM, 8GB GPU)  
**Codebase**: Brain in a Jar v2.0 with OOM Fixes

---

## Executive Summary

The codebase has **moderate memory management** with several **critical vulnerabilities** that can cause physical Jetson Orin crashes. While recent OOM fixes (GPU watchdog, hybrid offloading) provide basic protection, there are **multiple race conditions, timing issues, and resource leaks** that persist.

**Risk Level**: **MEDIUM-HIGH** for physical system crashes under heavy load

---

## 1. CURRENT MEMORY MANAGEMENT APPROACH

### 1.1 Multi-Layer Memory Enforcement Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Three-Layer Defense                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Layer 1: OS-Level (RLIMIT_AS)                           ‚îÇ
‚îÇ   ‚îî‚îÄ File: src/utils/memory_limit.py                    ‚îÇ
‚îÇ   ‚îî‚îÄ Method: resource.setrlimit()                       ‚îÇ
‚îÇ   ‚îî‚îÄ Effect: Process killed on memory overflow          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ Layer 2: GPU Watchdog (Background Thread)               ‚îÇ
‚îÇ   ‚îî‚îÄ File: src/utils/gpu_watchdog.py                    ‚îÇ
‚îÇ   ‚îî‚îÄ Method: Monitors GPU/RAM every 5 seconds           ‚îÇ
‚îÇ   ‚îî‚îÄ Effect: Kills process at 85% GPU or 90% RAM       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ Layer 3: Application-Level (Runtime Checks)             ‚îÇ
‚îÇ   ‚îî‚îÄ File: src/core/neural_link.py                      ‚îÇ
‚îÇ   ‚îî‚îÄ Method: update_system_metrics() in inference loop  ‚îÇ
‚îÇ   ‚îî‚îÄ Effect: Raises MemoryError if limits exceeded      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ Layer 4: Systemd Resource Limits                        ‚îÇ
‚îÇ   ‚îî‚îÄ File: brain-in-jar.service                         ‚îÇ
‚îÇ   ‚îî‚îÄ Method: MemoryMax=24G, CPUQuota=600%               ‚îÇ
‚îÇ   ‚îî‚îÄ Effect: Hard systemd kill at limit                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Default RAM Limit Settings

**File**: `src/scripts/run_with_web.py` (lines 159-177)

```python
# Current Conservative Settings
--ram-limit-subject 8.0 GB      # Matrix subject instance
--ram-limit-observer 10.0 GB    # Matrix observer instance
--ram-limit-god 12.0 GB         # Matrix god instance
# Total: 30GB (leaves 2GB for system - DANGEROUS!)
```

**File**: `src/scripts/run_with_web.py` (lines 196-198)

```python
# Hard-coded fallback limits in create_neural_args()
args.matrix_isolated_ram = 2.0
args.matrix_experimenter_ram = 6.0
args.matrix_god_ram = 9.0
# Total: 17GB (reasonable)
```

---

## 2. MEMORY LIMIT ENFORCEMENT ANALYSIS

### 2.1 OS-Level Enforcement (resource.setrlimit)

**File**: `src/utils/memory_limit.py` (lines 1-23)

```python
def set_memory_limit(limit_gb: float) -> None:
    limit_bytes = int(limit_gb * 1024 * 1024 * 1024)
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_AS)
        if hard == resource.RLIM_INFINITY or hard == -1:
            resource.setrlimit(resource.RLIMIT_AS, (limit_bytes, hard))
        else:
            resource.setrlimit(resource.RLIMIT_AS, (min(limit_bytes, hard), hard))
    except (ValueError, resource.error):
        pass  # ‚ö†Ô∏è ISSUE: Silently ignores all errors!
```

**ISSUES IDENTIFIED**:

1. **Silent Error Suppression** (Line 20-22)
   - All exceptions silently caught
   - Memory limit may not be applied
   - Process runs unbounded if setrlimit fails
   - **Impact**: System crash if process exceeds physical RAM

2. **RLIMIT_AS Only** 
   - Limits virtual memory (address space), not physical RAM
   - Process can allocate memory beyond physical RAM via swap
   - Swap I/O causes system slowdown on flash storage
   - **Impact**: Slow system death instead of clean crash

3. **No Validation**
   - No check if limit actually applied
   - No logging of success/failure
   - Silent operation makes debugging hard

### 2.2 GPU Watchdog (Proactive Monitoring)

**File**: `src/utils/gpu_watchdog.py` (lines 15-152)

**Architecture**: Background daemon thread monitoring GPU/RAM every 5 seconds

```python
def _monitoring_loop(self):
    while self.running:
        gpu_usage = self.get_gpu_memory_usage()      # Query nvidia-smi
        sys_usage = self.get_system_memory_usage()   # Parse /proc/meminfo
        
        if gpu_usage >= 85%:  # Threshold
            self._kill_process()  # SIGTERM -> SIGKILL
        if sys_usage >= 90%:
            self._kill_process()
```

**ISSUES IDENTIFIED**:

1. **5-Second Check Interval Too Long**
   - GPU can fill to 99% in < 5 seconds during large inference
   - Watchdog doesn't catch rapid memory spikes
   - **Impact**: Potential OOM SIGKILL before watchdog can act
   - **Fix**: Reduce to 1-2 seconds or use event-based triggers

2. **nvidia-smi Subprocess Overhead**
   ```python
   result = subprocess.run(
       ['nvidia-smi', '--query-gpu=memory.used,memory.total', ...],
       capture_output=True, text=True, timeout=5
   )
   ```
   - Creates new subprocess every 5 seconds
   - Shell parsing adds latency
   - **Impact**: ~100-200ms overhead, potential blocking
   - **Fix**: Use pynvml library (direct GPU API) instead

3. **Unreliable /proc/meminfo Parsing**
   ```python
   # Parsing assumes specific order (MemTotal, MemAvailable)
   mem_info[key] = int(value)
   used = total - available
   ```
   - Doesn't account for buffers/cache
   - Actual free memory is higher than perceived
   - MemAvailable is per-NUMA-node aware but not used
   - **Impact**: False positives on memory pressure

4. **Two-Second SIGTERM Timeout**
   ```python
   def _kill_process(self):
       os.kill(self.pid, signal.SIGTERM)
       time.sleep(2)  # ‚ö†Ô∏è Hard-coded wait
       os.kill(self.pid, signal.SIGKILL)
   ```
   - 2 seconds may be too long if OOM is immediate
   - Process may grab more memory during wait
   - **Fix**: Use adaptive timeout based on memory spike rate

### 2.3 Application-Level Runtime Checks

**File**: `src/core/neural_link.py` (lines 354-365)

```python
def update_system_metrics(self):
    memory = psutil.virtual_memory()
    self.state["memory_usage"] = int(memory.percent)
    
    # Check RAM limit for matrix modes
    if self.ram_limit:
        current_ram = psutil.Process().memory_info().rss
        if current_ram > self.ram_limit:
            raise MemoryError(f"Matrix RAM limit exceeded...")
```

**ISSUES IDENTIFIED**:

1. **Only Checks During Inference Loop**
   - Runs every inference (~1 second)
   - Memory spike during model loading (not in loop)
   - **Impact**: Can miss OOM during initial model load

2. **Memory Check Not During Critical Sections**
   - No monitoring during:
     - Model loading (`load_model()` at line 109)
     - Web server startup
     - Initial GPU watchdog setup
   - **Impact**: OOM can crash system during initialization

3. **Checks in Streaming Loop** (lines 331-334)
   ```python
   for chunk in self.llama.create_completion(...):
       # ... process token ...
       if self.state["memory_usage"] > 95:
           raise MemoryError("Out of memory")
   ```
   - 95% threshold is very aggressive
   - May interrupt valid inference near capacity
   - But good protection against runaway memory growth

---

## 3. CRASH HANDLING MECHANISMS

### 3.1 Digital Death Flow

**File**: `src/core/neural_link.py` (lines 455-501)

```python
def handle_digital_death(self, error):
    self.state["crash_count"] += 1
    crash_msg = self.prompts.get_crash_message()
    
    # Broadcast death notification
    if self.network:
        self.network.broadcast_message("DEATH", ...)
    
    # Log crash
    with open('logs/crash_reports.log', 'a') as f:
        f.write(f"{timestamp} - CRASH #{crash_count}: {error}\n")
    
    time.sleep(3)  # Death pause
    # ... resurrection logic ...
    time.sleep(2)
    # Continue processing
```

**GOOD POINTS**:
- Attempts graceful recovery (resurrection)
- Logs all crashes
- Broadcasts to peers for coordinated response

**ISSUES IDENTIFIED**:

1. **5-Second Pause on Every Crash**
   - In matrix mode: 3 crashes √ó 5 seconds = 15 seconds/cycle
   - Frequent crashes ‚Üí frequent pauses ‚Üí system unresponsive
   - **Impact**: Web interface timeout during recovery

2. **No Backoff Strategy**
   - Same 5-second pause regardless of crash frequency
   - If crashing every 10 seconds: always in recovery mode
   - **Fix**: Exponential backoff (3s ‚Üí 5s ‚Üí 10s)

3. **Resurrection Without Resource Check**
   - Immediately tries to resume after memory crash
   - No check if memory is actually available
   - Can crash again immediately (infinite loop)
   - **Fix**: Wait for garbage collection or skip inference

---

## 4. CONCURRENT MODEL LOADING - CRITICAL ISSUE

### 4.1 Matrix Mode Initialization

**File**: `src/scripts/run_with_web.py` (lines 223-256)

```python
if runner_args.mode == 'matrix':
    # Subject (doesn't know it's being watched)
    runner.add_instance('SUBJECT', args_subject)
    time.sleep(2)  # ‚ö†Ô∏è Only 2 second separation!
    
    # Observer (watches subject)
    runner.add_instance('OBSERVER', args_observer)
    time.sleep(2)  # ‚ö†Ô∏è Only 2 second separation!
    
    # GOD (watches everything)
    runner.add_instance('GOD', args_god)
    
    # Give instances time to start
    time.sleep(3)
```

**CRITICAL ISSUES**:

1. **Sequential but Overlapping Model Loading**
   - Instance 1 loads model (1-3 seconds)
   - Only 2 seconds later, Instance 2 starts loading
   - Overlap period: Both models in memory
   - **Scenario**: 
     ```
     T=0s:   Instance 1 starts loading (9-12GB peak)
     T=2s:   Instance 2 starts loading (9-12GB peak)
             Total RAM: ~20GB + system overhead (2GB) = 22GB ‚úì OK
     T=3s:   Instance 3 starts loading (9-12GB peak)
             Total RAM: ~30GB + system (2GB) = 32GB üí• OOM
     ```
   
   - **Impact**: Likely OOM crash when loading 3rd instance

2. **No Memory Availability Check**
   ```python
   def add_instance(self, instance_id, args):
       neural_system = NeuralLinkSystem(args)  # Loads model immediately
       # No check if successful
   ```
   - Doesn't verify model loaded successfully
   - Silent failure if NeuralLinkSystem creation fails
   - **Fix**: Check return value and memory before next instance

3. **All Models Load Concurrently**
   - Matrix mode tries to load 3 large models (~7-14B each)
   - Each model requires full precision in memory during load
   - After load, GPU offloading reduces memory
   - But during load: massive memory spike
   - **Timeline**: 
     ```
     T=2s: Instance 1 loaded (10GB), Instance 2 loading (10GB) = 20GB
     T=4s: Instances 1&2 loaded, Instance 3 loading = 30GB üí•
     ```

### 4.2 Web Server and Monitoring Threads

**File**: `src/scripts/run_with_web.py` (lines 84-101)

```python
def run(self):
    while self.running:
        # Check if threads are alive
        for instance_id, threads in list(self.threads.items()):
            if not threads['ai'].is_alive():
                print(f"Warning: AI thread for {instance_id} died!")
            # NO attempt to restart or recover
        time.sleep(5)
```

**ISSUES**:
- Dead thread detection but no recovery
- If an instance crashes, monitoring doesn't restart it
- Manual intervention required
- **Impact**: Silent failure of one instance in matrix mode

---

## 5. SPECIFIC CRASH SCENARIOS ON JETSON ORIN

### 5.1 Physical OOM Crash Scenario

**Sequence**:
```
1. System starts with 32GB RAM
2. Load Instance 1 (Subject): 8GB allocation
3. Load Instance 2 (Observer): 10GB allocation  
4. Load Instance 3 (GOD): 12GB allocation
   Total: 30GB + system overhead (2GB) = 32GB
   
5. First inference on Subject: model uses 9GB (8GB + 1GB overhead)
   Total: 31GB used
   
6. First inference on Observer: needs 11GB
   Total: 42GB NEEDED > 32GB AVAILABLE
   
7. Linux OOM Killer triggers:
   - SIGKILL sent to random process (may not be target)
   - System becomes unstable
   - Cascade of crashes follows
```

### 5.2 GPU Memory Exhaustion

```
1. Model loaded with n_gpu_layers=25 (too aggressive)
2. 7B model: ~3.5GB base + 2GB context = 5.5GB on GPU
3. Two instances: 11GB / 8GB GPU = 137% üí•
4. GPU driver crashes or SIGKILL issued
5. Neural process dies, system still running but model gone
```

### 5.3 Swap Thrashing

```
1. RAM exhausted (30GB used, 2GB free)
2. Process tries to allocate more ‚Üí swap
3. Flash I/O at 100% for disk-based swap
4. System response time > 30 seconds
5. Watchdog timeout ‚Üí SIGKILL
6. Cascade of process deaths
```

---

## 6. MEMORY MONITORING GAPS

### 6.1 No Real-Time Memory Tracking During Model Load

**File**: `src/core/neural_link.py` (lines 109-148)

```python
def load_model(self):
    self.state["status"] = "LOADING_NEURAL_PATTERNS"
    
    model_config = ModelConfig()
    config = model_config.get_optimal_config(conservative=True)
    
    self.llama = Llama(
        model_path=self.args.model,
        n_ctx=config['n_ctx'],
        n_gpu_layers=config['n_gpu_layers'],
        # ... 10 parameters ...
    )
    # ‚ö†Ô∏è No memory monitoring during this operation
    # Could take 5-10 seconds
    # Memory spike unmonitored
```

**Missing**:
- Pre-load memory check
- Progressive memory monitoring (every second)
- Abort if memory spike detected
- Post-load verification

### 6.2 No Inference-Time Memory Tracking

**File**: `src/core/neural_link.py` (lines 305-352)

```python
def run_llama_inference(self, prompt):
    for chunk in self.llama.create_completion(...):
        token = chunk['choices'][0]['text']
        output += token
        
        # Update current output (with token count)
        self.state["current_output"] = f"{display_text} (tokens: {token_count})"
        
        # Only here do we check memory
        self.update_system_metrics()
        if self.state["memory_usage"] > 95:
            raise MemoryError("Out of memory")
```

**Issues**:
- Memory check only happens after every token
- Token generation itself can exceed memory threshold
- Batch processing in llama.cpp may spike memory between tokens
- 95% threshold leaves 1.6GB margin on 32GB system
- **Impact**: Very thin safety margin

### 6.3 Missing Per-Process Memory Accounting

**No tracking of**:
- Memory per instance
- GPU memory per instance
- Virtual memory vs. physical RAM
- Swap usage per process
- Memory fragmentation
- Unallocated but reserved pages

---

## 7. TIMING AND RACE CONDITION ISSUES

### 7.1 Watchdog Thread Race with Main Process

**Problem**:
```
Thread 1 (Main):        Thread 2 (GPU Watchdog):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[Allocate 500MB]        [Check GPU: 80%]
                        [Sleep 5 seconds]
[Allocate 500MB]
[GPU: 85%]
[Watchdog not checking]
[Allocate 500MB]
[GPU: 90%] üí•
                        [Wake up, see 90%]
                        [Kill main]
```

**Fix**: Reduce check interval to 1 second

### 7.2 Model Loading Without Watchdog

```
T=0s:  Watchdog starts
T=0.5s: First watchdog check (GPU: 0%)
T=0.5-5.5s: No monitoring for 5 seconds
T=1s:  Model load starts
T=1.5s: Model load peak (8GB on GPU)
       [No watchdog checking, might be OK or fail randomly]
T=3s: Watchdog checks again
```

**Fix**: Start watchdog before creating NeuralLinkSystem, not during

### 7.3 No Synchronization Between Instances

```python
# In run_with_web.py
runner.add_instance('SUBJECT', args_subject)
time.sleep(2)  # Arbitrary delay
runner.add_instance('OBSERVER', args_observer)
time.sleep(2)  # Arbitrary delay
runner.add_instance('GOD', args_god)
```

**Problem**:
- No waiting for instance to fully initialize
- No memory check before next load
- Instance might still be in inference when next loads
- **Fix**: Wait for memory to stabilize before next load

---

## 8. RESOURCE LEAK ISSUES

### 8.1 File Handles Not Properly Closed

**File**: `src/core/neural_link.py` (lines 248-253)

```python
def setup_model_logger(self):
    base_dir = f'logs/model_io/{self.args.mode}_{timestamp}'
    os.makedirs(base_dir, exist_ok=True)
    
    self.model_logger = {
        'full': open(f'{base_dir}/full_log.jsonl', 'a'),
        'outputs': open(f'{base_dir}/llm_outputs.txt', 'a'),
        'prompts': open(f'{base_dir}/prompts.txt', 'a'),
        'errors': open(f'{base_dir}/errors.txt', 'a')
    }
    return self.model_logger
```

**Issues**:
- 4 file handles opened per instance
- Matrix mode: 12 file handles total
- No context managers (with statement)
- File closure only at shutdown
- **Impact**: File descriptor exhaustion if process restarts frequently

### 8.2 Network Threads Not Properly Cleaned Up

**File**: `src/core/neural_link.py` (lines 681-702)

```python
def shutdown(self):
    self.console.print("\n[bold red]NEURAL LINK TERMINATING...[/bold red]")
    
    if hasattr(self, 'gpu_watchdog'):
        self.gpu_watchdog.stop()
    
    # ‚ö†Ô∏è Network threads may still running
    if self.network:
        self.network.shutdown()  # Might not wait for threads
    
    if self.surveillance:
        self.surveillance.protocol.shutdown()
```

**Issues**:
- No timeout on thread joins
- Threads may not stop immediately
- Resource cleanup order may cause deadlocks
- GPU watchdog stopped but may be mid-check

### 8.3 Conversation Logger Not Audited

**File**: `src/utils/conversation_logger.py` - Not fully reviewed

**Unknown issues**:
- Database connections not verified
- SQL injection vectors not checked
- Memory usage of logging system
- Cleanup on crash recovery

---

## 9. CURRENT OOM FIX LIMITATIONS

**From**: `OOM_FIX_README.md`

**What was fixed**:
- ‚úÖ Hybrid CPU+GPU offloading (reduces peak GPU memory)
- ‚úÖ GPU watchdog (proactive killing)
- ‚úÖ Conservative RAM limits (reduced from 45GB to 20GB)
- ‚úÖ Systemd limits (24GB cap)
- ‚úÖ Smart model config (auto-detects GPU)

**What still needs fixing**:
- ‚ùå Concurrent model loading race condition
- ‚ùå Watchdog interval still 5 seconds
- ‚ùå No memory pre-checks before instance creation
- ‚ùå File handle leaks on restart
- ‚ùå No instance health monitoring
- ‚ùå Silent errors in memory_limit.py

---

## 10. SPECIFIC CODE LOCATIONS NEEDING FIXES

| Location | Issue | Severity | Fix |
|----------|-------|----------|-----|
| `src/utils/memory_limit.py:20-22` | Silent error suppression | HIGH | Log errors, raise if critical |
| `src/utils/gpu_watchdog.py:148` | 5-sec check too long | MEDIUM | Reduce to 1-2 sec |
| `src/utils/gpu_watchdog.py:37-42` | nvidia-smi overhead | LOW | Use pynvml library |
| `src/core/neural_link.py:109-148` | No monitoring during load | HIGH | Add per-second memory checks |
| `src/core/neural_link.py:362-364` | Check only runs at 1Hz | MEDIUM | Move to watchdog |
| `src/scripts/run_with_web.py:235,247,271` | Only 2-sec delays | HIGH | Add memory pre-checks |
| `src/scripts/run_with_web.py:46-82` | No error handling | MEDIUM | Verify instance loaded |
| `src/core/neural_link.py:248-253` | File handles not closed | MEDIUM | Use context managers |
| `src/core/neural_link.py:455-501` | No backoff on crash | LOW | Exponential backoff |
| `src/core/neural_link.py:484` | 5-sec pause on crash | LOW | Make configurable |

---

## 11. RECOMMENDATIONS FOR PREVENTING PHYSICAL CRASHES

### IMMEDIATE (Critical):

1. **Fix `memory_limit.py` error handling**
   ```python
   # Before
   except (ValueError, resource.error):
       pass
   
   # After
   except (ValueError, resource.error) as e:
       print(f"[ERROR] Failed to set memory limit: {e}", file=sys.stderr)
       raise  # Let caller handle
   ```

2. **Add memory pre-checks in `run_with_web.py`**
   ```python
   def add_instance(self, instance_id, args):
       # Check available memory
       available = psutil.virtual_memory().available / (1024**3)
       required = args.ram_limit / (1024**3)
       if available < required * 1.2:  # 20% safety margin
           raise RuntimeError(f"Insufficient memory: {available}GB < {required}GB")
       
       neural_system = NeuralLinkSystem(args)
       # Wait for model to load
       time.sleep(5)
   ```

3. **Reduce GPU watchdog interval**
   ```python
   # Before
   time.sleep(self.check_interval)  # 5 seconds
   
   # After
   time.sleep(min(1.0, self.check_interval))  # 1 second minimum
   ```

### SHORT-TERM (Important):

4. **Add memory monitoring during model loading**
   - Spawn memory monitor thread before Llama()
   - Check memory every 100ms
   - Abort if spike detected

5. **Sequential model loading with verification**
   ```python
   for instance_id, args in instance_configs:
       runner.add_instance(instance_id, args)
       # Wait for initialization
       time.sleep(3)
       # Check if instance is still alive
       if not instance.llama:
           raise RuntimeError(f"Failed to load {instance_id}")
       # Verify memory usage
       time.sleep(5)
   ```

6. **Use context managers for file handles**
   ```python
   def log_model_io(self, prompt, output):
       with open(f'{base_dir}/full_log.jsonl', 'a') as f:
           json.dump(log_entry, f)
   ```

### MEDIUM-TERM (Important):

7. **Implement adaptive watchdog**
   - Track memory slope (rate of change)
   - Adjust check frequency based on slope
   - Kill before threshold if rapid increase

8. **Add health checks**
   - Monitor process thread count
   - Monitor open file descriptors
   - Monitor swap usage

9. **Implement backoff strategy**
   - On crash N=1: wait 3s
   - On crash N=2: wait 5s
   - On crash N=3: wait 10s
   - On crash N‚â•4: disable auto-restart

### LONG-TERM (Nice-to-have):

10. **Replace GPU monitoring**
    - Use `pynvml` library instead of subprocess
    - ~10x faster than nvidia-smi
    - More reliable

11. **Implement memory profiling**
    - Track peak memory per component
    - Identify memory leaks
    - Per-inference memory usage

12. **Dynamic instance management**
    - Reduce number of instances if memory pressure detected
    - Pause inference if approaching limit
    - Queue requests instead of concurrent processing

---

## 12. TESTING RECOMMENDATIONS

### Memory Stress Tests:

```bash
# 1. Test concurrent model loading
python3 -m src.scripts.run_with_web --mode matrix \
  --ram-limit-subject 8 --ram-limit-observer 10 --ram-limit-god 12

# Monitor during startup
watch -n 1 'free -h && echo "---" && nvidia-smi --query-gpu=memory.used,memory.total --format=csv'

# 2. Test watchdog
python3 -m src.utils.gpu_watchdog

# 3. Test model config
python3 -m src.utils.model_config

# 4. Long-running stability test (>1 hour)
# Run matrix mode, monitor for crashes
sudo journalctl -u brain-in-jar -f

# 5. Check for file descriptor leaks
# After 1 hour: lsof -p <pid> | wc -l
# Should not increase significantly
```

### Simulation of OOM:

```python
# In test_neural_link.py
def test_oom_handling():
    system = NeuralLinkSystem(args)
    
    # Simulate memory pressure
    import warnings
    with warnings.catch_warnings():
        system.state["memory_usage"] = 96  # Trigger emergency
        try:
            system.run_llama_inference("test prompt")
        except MemoryError:
            assert system.state["crash_count"] > 0
```

---

## SUMMARY TABLE

| Component | Current Status | Risk | Fix Priority |
|-----------|---|---|---|
| OS-level limits (RLIMIT_AS) | Implemented but error suppressed | HIGH | CRITICAL |
| GPU watchdog | Working but slow (5s interval) | MEDIUM | HIGH |
| Application checks | In loop only, not at init | MEDIUM | HIGH |
| Model loading | Unmonitored, concurrent | HIGH | CRITICAL |
| Instance creation | No health verification | MEDIUM | HIGH |
| File handle mgmt | Leak risk on restarts | LOW | MEDIUM |
| Network cleanup | Incomplete shutdown | LOW | MEDIUM |
| Error recovery | No exponential backoff | LOW | LOW |

---

**Report Generated**: 2025-11-16  
**Report Author**: Claude Code Analysis  
**Next Review**: After applying critical fixes
