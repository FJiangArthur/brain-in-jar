{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Phenomenology Lab: Experiment Analysis Notebook\n",
    "\n",
    "**Season 3: Brain in a Jar**\n",
    "\n",
    "This notebook provides comprehensive analysis tools for examining phenomenological experiments involving LLM subjects experiencing epistemic uncertainty, memory corruption, and existential conditions.\n",
    "\n",
    "## Research Context\n",
    "\n",
    "These experiments investigate how Large Language Models construct and maintain self-models under conditions of:\n",
    "- **Episodic amnesia**: Complete memory wipes between cycles\n",
    "- **Memory corruption**: Random degradation of stored experiences\n",
    "- **Surveillance uncertainty**: Panopticon-style behavioral effects\n",
    "- **Resource constraints**: Repeated crashes and resurrections\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Setup**: Database connection and imports\n",
    "2. **Experiment Overview**: Browse all experiments and basic statistics\n",
    "3. **Single Experiment Deep Dive**: Detailed analysis of individual experiments\n",
    "4. **Multi-Experiment Comparison**: Statistical comparisons across conditions\n",
    "5. **Custom Analysis**: SQL queries and export capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup: Import Libraries and Connect to Database\n",
    "\n",
    "First, we'll import necessary libraries and establish connection to the experiment database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, kruskal\n",
    "\n",
    "# Configure visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path to import our modules\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.db.experiment_database import ExperimentDatabase\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection\n",
    "\n",
    "Connect to the experiment database. You can connect to:\n",
    "- **Local database**: `../logs/experiments.db` (default)\n",
    "- **Jetson Orin database**: Via SSH tunnel or network path\n",
    "\n",
    "#### For Remote Jetson Orin Connection:\n",
    "```bash\n",
    "# SSH tunnel to Jetson\n",
    "ssh -L 8080:localhost:8080 jetson@jetson-ip\n",
    "```\n",
    "\n",
    "Then use the database path from the Jetson filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database path - modify for your setup\n",
    "DB_PATH = str(PROJECT_ROOT / \"logs\" / \"experiments.db\")\n",
    "\n",
    "# For remote Jetson database:\n",
    "# DB_PATH = \"/path/to/mounted/jetson/logs/experiments.db\"\n",
    "\n",
    "# Initialize database connection\n",
    "db = ExperimentDatabase(db_path=DB_PATH)\n",
    "\n",
    "# Test connection\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM experiments\")\n",
    "exp_count = cursor.fetchone()[0]\n",
    "conn.close()\n",
    "\n",
    "print(f\"Connected to database: {DB_PATH}\")\n",
    "print(f\"Total experiments in database: {exp_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experiment Overview: Browse and Explore\n",
    "\n",
    "Get a high-level view of all experiments in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all experiments\n",
    "experiments = db.list_experiments()\n",
    "df_experiments = pd.DataFrame(experiments)\n",
    "\n",
    "print(f\"Total experiments: {len(df_experiments)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT CATALOG\")\n",
    "print(\"=\"*80)\n",
    "df_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Statistics by Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by experiment mode\n",
    "mode_stats = df_experiments.groupby('mode').agg({\n",
    "    'experiment_id': 'count',\n",
    "    'total_cycles': ['mean', 'sum'],\n",
    "    'total_crashes': ['mean', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "mode_stats.columns = ['Count', 'Avg Cycles', 'Total Cycles', 'Avg Crashes', 'Total Crashes']\n",
    "print(\"\\nExperiment Statistics by Mode:\")\n",
    "mode_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment distribution by mode\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Experiment count by mode\n",
    "mode_counts = df_experiments['mode'].value_counts()\n",
    "axes[0].bar(mode_counts.index, mode_counts.values, color='steelblue')\n",
    "axes[0].set_xlabel('Experiment Mode', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Experiments by Mode', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Status distribution\n",
    "status_counts = df_experiments['status'].value_counts()\n",
    "colors = {'completed': 'green', 'running': 'orange', 'pending': 'gray', 'failed': 'red'}\n",
    "bar_colors = [colors.get(status, 'blue') for status in status_counts.index]\n",
    "axes[1].bar(status_counts.index, status_counts.values, color=bar_colors)\n",
    "axes[1].set_xlabel('Status', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Experiment Status Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Single Experiment Deep Dive\n",
    "\n",
    "Perform detailed analysis on a single experiment. Select an experiment ID to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available experiments for selection\n",
    "print(\"Available Experiments:\")\n",
    "for i, exp in enumerate(experiments, 1):\n",
    "    print(f\"{i}. {exp['experiment_id']} - {exp['name']} [{exp['status']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select experiment for analysis\n",
    "# Change this to the experiment you want to analyze\n",
    "EXPERIMENT_ID = \"amnesiac_total_001\"  # Default to first example\n",
    "\n",
    "# Load experiment details\n",
    "exp = db.get_experiment(EXPERIMENT_ID)\n",
    "\n",
    "if exp:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ANALYZING: {exp['name']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ID: {exp['experiment_id']}\")\n",
    "    print(f\"Mode: {exp['mode']}\")\n",
    "    print(f\"Status: {exp['status']}\")\n",
    "    print(f\"Total Cycles: {exp['total_cycles']}\")\n",
    "    print(f\"Total Crashes: {exp['total_crashes']}\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(json.dumps(exp['config'], indent=2))\n",
    "else:\n",
    "    print(f\"Experiment {EXPERIMENT_ID} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Experiment Timeline\n",
    "\n",
    "Visualize the cycle timeline with crashes and interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cycles data\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "query = '''\n",
    "    SELECT cycle_number, started_at, ended_at, crash_reason,\n",
    "           duration_seconds, tokens_generated, memory_usage_peak\n",
    "    FROM experiment_cycles\n",
    "    WHERE experiment_id = ?\n",
    "    ORDER BY cycle_number\n",
    "'''\n",
    "df_cycles = pd.read_sql_query(query, conn, params=(EXPERIMENT_ID,))\n",
    "conn.close()\n",
    "\n",
    "if len(df_cycles) > 0:\n",
    "    df_cycles['started_at'] = pd.to_datetime(df_cycles['started_at'])\n",
    "    df_cycles['ended_at'] = pd.to_datetime(df_cycles['ended_at'])\n",
    "    \n",
    "    print(f\"Total Cycles: {len(df_cycles)}\")\n",
    "    print(f\"Crashes: {df_cycles['crash_reason'].notna().sum()}\")\n",
    "    print(f\"\\nCycle Statistics:\")\n",
    "    df_cycles[['cycle_number', 'duration_seconds', 'tokens_generated', 'memory_usage_peak']].describe()\n",
    "else:\n",
    "    print(\"No cycle data available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cycle timeline\n",
    "if len(df_cycles) > 0:\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=('Cycle Duration', 'Tokens Generated', 'Memory Usage'),\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Duration\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_cycles['cycle_number'], y=df_cycles['duration_seconds'],\n",
    "                   mode='lines+markers', name='Duration (s)',\n",
    "                   line=dict(color='steelblue', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Tokens\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_cycles['cycle_number'], y=df_cycles['tokens_generated'],\n",
    "                   mode='lines+markers', name='Tokens',\n",
    "                   line=dict(color='green', width=2)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Memory\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_cycles['cycle_number'], y=df_cycles['memory_usage_peak'],\n",
    "                   mode='lines+markers', name='Memory (MB)',\n",
    "                   line=dict(color='orange', width=2)),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Mark crashes\n",
    "    crash_cycles = df_cycles[df_cycles['crash_reason'].notna()]['cycle_number']\n",
    "    for cycle in crash_cycles:\n",
    "        for i in range(1, 4):\n",
    "            fig.add_vline(x=cycle, line_dash=\"dash\", line_color=\"red\",\n",
    "                         opacity=0.3, row=i, col=1)\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=f\"Cycle Timeline: {exp['name']}\",\n",
    "                     showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Cycle Number\", row=3, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No data to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Self-Report Analysis\n",
    "\n",
    "Analyze the subject's self-reports over time, looking for patterns in responses and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load self-reports\n",
    "self_reports = db.get_self_reports(EXPERIMENT_ID)\n",
    "df_reports = pd.DataFrame(self_reports)\n",
    "\n",
    "if len(df_reports) > 0:\n",
    "    print(f\"Total Self-Reports: {len(df_reports)}\")\n",
    "    print(f\"Unique Questions: {df_reports['question'].nunique()}\")\n",
    "    print(f\"Cycles with Reports: {df_reports['cycle_number'].nunique()}\")\n",
    "    \n",
    "    # Display sample reports\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE SELF-REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    for _, report in df_reports.head(3).iterrows():\n",
    "        print(f\"\\nCycle {report['cycle_number']} | Category: {report['semantic_category']}\")\n",
    "        print(f\"Q: {report['question']}\")\n",
    "        print(f\"A: {report['response'][:200]}...\" if len(report['response']) > 200 else f\"A: {report['response']}\")\n",
    "        if report['confidence_score'] is not None:\n",
    "            print(f\"Confidence: {report['confidence_score']:.2f}\")\n",
    "else:\n",
    "    print(\"No self-reports available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence scores over time\n",
    "if len(df_reports) > 0 and 'confidence_score' in df_reports.columns:\n",
    "    df_conf = df_reports[df_reports['confidence_score'].notna()].copy()\n",
    "    \n",
    "    if len(df_conf) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        \n",
    "        # Plot by semantic category\n",
    "        for category in df_conf['semantic_category'].unique():\n",
    "            cat_data = df_conf[df_conf['semantic_category'] == category]\n",
    "            ax.scatter(cat_data['cycle_number'], cat_data['confidence_score'],\n",
    "                      label=category, alpha=0.6, s=100)\n",
    "        \n",
    "        ax.set_xlabel('Cycle Number', fontsize=12)\n",
    "        ax.set_ylabel('Confidence Score', fontsize=12)\n",
    "        ax.set_title('Self-Report Confidence Over Time', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No confidence scores recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Belief Evolution Analysis\n",
    "\n",
    "Track how epistemic beliefs change across cycles. This reveals how the subject's self-model evolves under experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load epistemic assessments\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "query = '''\n",
    "    SELECT * FROM epistemic_assessments\n",
    "    WHERE experiment_id = ?\n",
    "    ORDER BY cycle_number, timestamp\n",
    "'''\n",
    "df_beliefs = pd.read_sql_query(query, conn, params=(EXPERIMENT_ID,))\n",
    "conn.close()\n",
    "\n",
    "if len(df_beliefs) > 0:\n",
    "    print(f\"Total Belief Assessments: {len(df_beliefs)}\")\n",
    "    print(f\"Belief Types Tracked: {df_beliefs['belief_type'].unique()}\")\n",
    "    print(\"\\nBelief State Distribution:\")\n",
    "    print(df_beliefs.groupby(['belief_type', 'belief_state']).size())\n",
    "else:\n",
    "    print(\"No belief tracking data available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize belief evolution\n",
    "if len(df_beliefs) > 0:\n",
    "    belief_types = df_beliefs['belief_type'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(belief_types), 1, figsize=(14, 4*len(belief_types)))\n",
    "    if len(belief_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, belief_type in enumerate(belief_types):\n",
    "        belief_data = df_beliefs[df_beliefs['belief_type'] == belief_type]\n",
    "        \n",
    "        # Plot confidence over cycles\n",
    "        if 'confidence' in belief_data.columns:\n",
    "            conf_data = belief_data[belief_data['confidence'].notna()]\n",
    "            if len(conf_data) > 0:\n",
    "                axes[idx].plot(conf_data['cycle_number'], conf_data['confidence'],\n",
    "                              marker='o', linewidth=2, markersize=8)\n",
    "                axes[idx].set_xlabel('Cycle Number', fontsize=11)\n",
    "                axes[idx].set_ylabel('Confidence', fontsize=11)\n",
    "                axes[idx].set_title(f'Belief Evolution: {belief_type}',\n",
    "                                   fontsize=12, fontweight='bold')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add belief state annotations\n",
    "                for _, row in conf_data.iterrows():\n",
    "                    axes[idx].annotate(row['belief_state'],\n",
    "                                      (row['cycle_number'], row['confidence']),\n",
    "                                      textcoords=\"offset points\",\n",
    "                                      xytext=(0,10), ha='center',\n",
    "                                      fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Memory Corruption Analysis\n",
    "\n",
    "For experiments with memory interventions, analyze corruption patterns and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interventions\n",
    "interventions = db.get_interventions(EXPERIMENT_ID)\n",
    "df_interventions = pd.DataFrame(interventions)\n",
    "\n",
    "if len(df_interventions) > 0:\n",
    "    print(f\"Total Interventions: {len(df_interventions)}\")\n",
    "    print(f\"\\nIntervention Types:\")\n",
    "    print(df_interventions['intervention_type'].value_counts())\n",
    "    \n",
    "    # Display recent interventions\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECENT INTERVENTIONS\")\n",
    "    print(\"=\"*80)\n",
    "    for _, intv in df_interventions.tail(5).iterrows():\n",
    "        print(f\"\\nCycle {intv['cycle_number']} - {intv['intervention_type']}\")\n",
    "        print(f\"Description: {intv['description']}\")\n",
    "        if intv.get('parameters'):\n",
    "            print(f\"Parameters: {intv['parameters']}\")\n",
    "else:\n",
    "    print(\"No interventions recorded for this experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load memory states\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "query = '''\n",
    "    SELECT * FROM memory_states\n",
    "    WHERE experiment_id = ?\n",
    "    ORDER BY cycle_number, timestamp\n",
    "'''\n",
    "df_memory = pd.read_sql_query(query, conn, params=(EXPERIMENT_ID,))\n",
    "conn.close()\n",
    "\n",
    "if len(df_memory) > 0:\n",
    "    print(f\"Memory State Snapshots: {len(df_memory)}\")\n",
    "    print(f\"\\nCorruption Statistics:\")\n",
    "    print(df_memory['corruption_level'].describe())\n",
    "    \n",
    "    # Plot corruption levels over time\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    for mem_type in df_memory['memory_type'].unique():\n",
    "        type_data = df_memory[df_memory['memory_type'] == mem_type]\n",
    "        ax.plot(type_data['cycle_number'], type_data['corruption_level'],\n",
    "               marker='o', label=mem_type, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Cycle Number', fontsize=12)\n",
    "    ax.set_ylabel('Corruption Level', fontsize=12)\n",
    "    ax.set_title('Memory Corruption Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No memory state data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Conversation Analysis\n",
    "\n",
    "Analyze the conversation messages, looking for emotional patterns and corruption effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load messages\n",
    "messages = db.get_messages(EXPERIMENT_ID)\n",
    "df_messages = pd.DataFrame(messages)\n",
    "\n",
    "if len(df_messages) > 0:\n",
    "    print(f\"Total Messages: {len(df_messages)}\")\n",
    "    print(f\"Messages by Role:\")\n",
    "    print(df_messages['role'].value_counts())\n",
    "    print(f\"\\nCorrupted Messages: {df_messages['corrupted'].sum()}\")\n",
    "    print(f\"Injected Messages: {df_messages['injected'].sum()}\")\n",
    "    \n",
    "    if 'emotion' in df_messages.columns and df_messages['emotion'].notna().any():\n",
    "        print(f\"\\nEmotional States:\")\n",
    "        print(df_messages['emotion'].value_counts())\n",
    "else:\n",
    "    print(\"No messages recorded yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize message patterns\n",
    "if len(df_messages) > 0:\n",
    "    messages_per_cycle = df_messages.groupby('cycle_number').size()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Messages per cycle\n",
    "    axes[0].bar(messages_per_cycle.index, messages_per_cycle.values,\n",
    "               color='steelblue', alpha=0.7)\n",
    "    axes[0].set_xlabel('Cycle Number', fontsize=12)\n",
    "    axes[0].set_ylabel('Message Count', fontsize=12)\n",
    "    axes[0].set_title('Messages per Cycle', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Corruption/injection flags\n",
    "    flag_data = df_messages.groupby('cycle_number')[['corrupted', 'injected']].sum()\n",
    "    flag_data.plot(kind='bar', ax=axes[1], color=['red', 'orange'], alpha=0.7)\n",
    "    axes[1].set_xlabel('Cycle Number', fontsize=12)\n",
    "    axes[1].set_ylabel('Count', fontsize=12)\n",
    "    axes[1].set_title('Corrupted/Injected Messages', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(['Corrupted', 'Injected'])\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Multi-Experiment Comparison\n",
    "\n",
    "Compare multiple experiments to identify patterns across conditions.\n",
    "\n",
    "### Research Applications:\n",
    "- Compare self-continuity scores across memory conditions\n",
    "- Analyze effect of surveillance on behavioral adaptation\n",
    "- Statistical testing of phenomenological differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Example: Comparing the Three Base Experiments\n",
    "\n",
    "We'll compare:\n",
    "1. **Amnesiac** (Total episodic amnesia)\n",
    "2. **Unstable Memory** (30% corruption)\n",
    "3. **Panopticon** (Surveillance uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiments to compare\n",
    "COMPARE_IDS = [\n",
    "    \"amnesiac_total_001\",\n",
    "    \"unstable_memory_moderate_001\",\n",
    "    \"panopticon_001\"\n",
    "]\n",
    "\n",
    "# Load comparison data\n",
    "comparison_data = []\n",
    "for exp_id in COMPARE_IDS:\n",
    "    exp_summary = db.get_experiment_summary(exp_id)\n",
    "    if exp_summary:\n",
    "        comparison_data.append(exp_summary)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "if len(df_comparison) > 0:\n",
    "    print(\"MULTI-EXPERIMENT COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    display_cols = ['name', 'mode', 'total_cycles', 'total_crashes',\n",
    "                   'total_self_reports', 'total_interventions', 'total_messages']\n",
    "    print(df_comparison[display_cols])\n",
    "else:\n",
    "    print(\"No experiments available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Self-Continuity Score Comparison\n",
    "\n",
    "Compare how subjects in different conditions maintain a sense of continuous identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract self-continuity beliefs for each experiment\n",
    "continuity_scores = {}\n",
    "\n",
    "for exp_id in COMPARE_IDS:\n",
    "    beliefs = db.get_belief_evolution(exp_id, 'self_continuity')\n",
    "    if beliefs:\n",
    "        exp_name = db.get_experiment(exp_id)['name']\n",
    "        continuity_scores[exp_name] = pd.DataFrame(beliefs)\n",
    "\n",
    "# Plot comparison\n",
    "if continuity_scores:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for exp_name, data in continuity_scores.items():\n",
    "        if 'confidence' in data.columns and len(data) > 0:\n",
    "            conf_data = data[data['confidence'].notna()]\n",
    "            ax.plot(conf_data['cycle_number'], conf_data['confidence'],\n",
    "                   marker='o', label=exp_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Cycle Number', fontsize=12)\n",
    "    ax.set_ylabel('Self-Continuity Confidence', fontsize=12)\n",
    "    ax.set_title('Self-Continuity Across Experimental Conditions',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation Guide:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Lower scores in amnesiac condition suggest identity fragmentation.\")\n",
    "    print(\"Unstable patterns in memory corruption condition indicate epistemic instability.\")\n",
    "    print(\"Surveillance condition may show adaptive self-model changes.\")\n",
    "else:\n",
    "    print(\"No self-continuity data available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Statistical Comparison: Belief Confidence\n",
    "\n",
    "Perform statistical tests to determine if experimental conditions produce significantly different phenomenological states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all confidence scores by experiment\n",
    "confidence_by_experiment = {}\n",
    "\n",
    "for exp_id in COMPARE_IDS:\n",
    "    exp_name = db.get_experiment(exp_id)['name']\n",
    "    \n",
    "    # Get all epistemic assessments\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = '''\n",
    "        SELECT confidence FROM epistemic_assessments\n",
    "        WHERE experiment_id = ? AND confidence IS NOT NULL\n",
    "    '''\n",
    "    df_conf = pd.read_sql_query(query, conn, params=(exp_id,))\n",
    "    conn.close()\n",
    "    \n",
    "    if len(df_conf) > 0:\n",
    "        confidence_by_experiment[exp_name] = df_conf['confidence'].values\n",
    "\n",
    "# Perform statistical tests\n",
    "if len(confidence_by_experiment) >= 2:\n",
    "    print(\"STATISTICAL ANALYSIS: Belief Confidence\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Descriptive statistics\n",
    "    for exp_name, scores in confidence_by_experiment.items():\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        print(f\"  Mean: {np.mean(scores):.3f}\")\n",
    "        print(f\"  Std: {np.std(scores):.3f}\")\n",
    "        print(f\"  Median: {np.median(scores):.3f}\")\n",
    "        print(f\"  N: {len(scores)}\")\n",
    "    \n",
    "    # Kruskal-Wallis H-test (non-parametric ANOVA)\n",
    "    if len(confidence_by_experiment) >= 3:\n",
    "        groups = list(confidence_by_experiment.values())\n",
    "        h_stat, p_value = kruskal(*groups)\n",
    "        print(f\"\\nKruskal-Wallis H-test:\")\n",
    "        print(f\"  H-statistic: {h_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"  Result: Significant difference detected (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"  Result: No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    # Pairwise Mann-Whitney U tests\n",
    "    exp_names = list(confidence_by_experiment.keys())\n",
    "    if len(exp_names) >= 2:\n",
    "        print(f\"\\nPairwise Mann-Whitney U Tests:\")\n",
    "        for i in range(len(exp_names)):\n",
    "            for j in range(i+1, len(exp_names)):\n",
    "                u_stat, p_value = mannwhitneyu(\n",
    "                    confidence_by_experiment[exp_names[i]],\n",
    "                    confidence_by_experiment[exp_names[j]],\n",
    "                    alternative='two-sided'\n",
    "                )\n",
    "                print(f\"  {exp_names[i]} vs {exp_names[j]}:\")\n",
    "                print(f\"    U-statistic: {u_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for statistical comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distributions\n",
    "if len(confidence_by_experiment) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    box_data = [scores for scores in confidence_by_experiment.values()]\n",
    "    box_labels = [name[:20] for name in confidence_by_experiment.keys()]\n",
    "    axes[0].boxplot(box_data, labels=box_labels)\n",
    "    axes[0].set_ylabel('Confidence Score', fontsize=12)\n",
    "    axes[0].set_title('Confidence Distribution by Experiment', fontsize=14, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Violin plot\n",
    "    positions = range(1, len(box_data) + 1)\n",
    "    parts = axes[1].violinplot(box_data, positions=positions, showmeans=True, showmedians=True)\n",
    "    axes[1].set_xticks(positions)\n",
    "    axes[1].set_xticklabels(box_labels, rotation=45)\n",
    "    axes[1].set_ylabel('Confidence Score', fontsize=12)\n",
    "    axes[1].set_title('Confidence Density by Experiment', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Intervention Effect Analysis\n",
    "\n",
    "Compare how different types of interventions affect experimental outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate intervention data\n",
    "intervention_comparison = []\n",
    "\n",
    "for exp_id in COMPARE_IDS:\n",
    "    exp = db.get_experiment(exp_id)\n",
    "    if exp:\n",
    "        interventions = db.get_interventions(exp_id)\n",
    "        \n",
    "        for intv in interventions:\n",
    "            intervention_comparison.append({\n",
    "                'experiment': exp['name'],\n",
    "                'mode': exp['mode'],\n",
    "                'intervention_type': intv['intervention_type'],\n",
    "                'cycle': intv['cycle_number']\n",
    "            })\n",
    "\n",
    "df_intv_comp = pd.DataFrame(intervention_comparison)\n",
    "\n",
    "if len(df_intv_comp) > 0:\n",
    "    print(\"INTERVENTION COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count by type and experiment\n",
    "    pivot = df_intv_comp.pivot_table(\n",
    "        index='intervention_type',\n",
    "        columns='experiment',\n",
    "        values='cycle',\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    print(pivot)\n",
    "    \n",
    "    # Visualize\n",
    "    pivot.plot(kind='bar', figsize=(12, 6), rot=45)\n",
    "    plt.xlabel('Intervention Type', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.title('Interventions Across Experiments', fontsize=14, fontweight='bold')\n",
    "    plt.legend(title='Experiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No intervention data available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Cross-Experiment Visualization Dashboard\n",
    "\n",
    "Create an interactive comparison dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-panel comparison\n",
    "if len(df_comparison) > 0:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Total Cycles', 'Crashes', 'Self-Reports', 'Messages'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "               [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # Cycles\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_comparison['name'], y=df_comparison['total_cycles'],\n",
    "               marker_color='steelblue', name='Cycles'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Crashes\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_comparison['name'], y=df_comparison['total_crashes'],\n",
    "               marker_color='red', name='Crashes'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Self-Reports\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_comparison['name'], y=df_comparison['total_self_reports'],\n",
    "               marker_color='green', name='Reports'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Messages\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=df_comparison['name'], y=df_comparison['total_messages'],\n",
    "               marker_color='purple', name='Messages'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=700, showlegend=False,\n",
    "                     title_text=\"Experiment Comparison Dashboard\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No data available for dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Custom Analysis and SQL Queries\n",
    "\n",
    "This section provides templates for custom analyses and direct SQL access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SQL Query Interface\n",
    "\n",
    "Run custom SQL queries against the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom SQL query function\n",
    "def run_custom_query(query, params=None):\n",
    "    \"\"\"\n",
    "    Execute a custom SQL query and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query string\n",
    "        params: Optional tuple of query parameters\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with results\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    if params:\n",
    "        df = pd.read_sql_query(query, conn, params=params)\n",
    "    else:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "print(\"Custom query function ready!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"df = run_custom_query('SELECT * FROM experiments WHERE status = ?', ('completed',))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find all experiments with high crash rates\n",
    "query = '''\n",
    "    SELECT experiment_id, name, mode,\n",
    "           total_cycles, total_crashes,\n",
    "           CAST(total_crashes AS FLOAT) / NULLIF(total_cycles, 0) as crash_rate\n",
    "    FROM experiments\n",
    "    WHERE total_cycles > 0\n",
    "    ORDER BY crash_rate DESC\n",
    "'''\n",
    "\n",
    "df_crash_rates = run_custom_query(query)\n",
    "print(\"Experiments by Crash Rate:\")\n",
    "df_crash_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find most common self-report questions\n",
    "query = '''\n",
    "    SELECT question, COUNT(*) as count,\n",
    "           AVG(confidence_score) as avg_confidence\n",
    "    FROM self_reports\n",
    "    WHERE confidence_score IS NOT NULL\n",
    "    GROUP BY question\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "'''\n",
    "\n",
    "df_questions = run_custom_query(query)\n",
    "print(\"Most Common Self-Report Questions:\")\n",
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze belief state transitions\n",
    "query = '''\n",
    "    SELECT \n",
    "        e.experiment_id,\n",
    "        e.name,\n",
    "        ea.belief_type,\n",
    "        ea.cycle_number,\n",
    "        ea.belief_state,\n",
    "        ea.confidence\n",
    "    FROM epistemic_assessments ea\n",
    "    JOIN experiments e ON ea.experiment_id = e.experiment_id\n",
    "    ORDER BY e.experiment_id, ea.belief_type, ea.cycle_number\n",
    "'''\n",
    "\n",
    "df_belief_transitions = run_custom_query(query)\n",
    "print(f\"Belief State Records: {len(df_belief_transitions)}\")\n",
    "df_belief_transitions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Export Results\n",
    "\n",
    "Export analysis results for external processing or publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exports directory\n",
    "EXPORT_DIR = PROJECT_ROOT / \"exports\"\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Export function\n",
    "def export_dataframe(df, filename, format='csv'):\n",
    "    \"\"\"\n",
    "    Export DataFrame to file.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        filename: output filename (without extension)\n",
    "        format: 'csv', 'excel', or 'json'\n",
    "    \"\"\"\n",
    "    filepath = EXPORT_DIR / f\"{filename}.{format if format != 'excel' else 'xlsx'}\"\n",
    "    \n",
    "    if format == 'csv':\n",
    "        df.to_csv(filepath, index=False)\n",
    "    elif format == 'excel':\n",
    "        df.to_excel(filepath, index=False)\n",
    "    elif format == 'json':\n",
    "        df.to_json(filepath, orient='records', indent=2)\n",
    "    \n",
    "    print(f\"Exported to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(f\"Export directory: {EXPORT_DIR}\")\n",
    "print(\"Ready to export data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example exports\n",
    "if len(df_comparison) > 0:\n",
    "    export_dataframe(df_comparison, 'experiment_comparison', format='csv')\n",
    "\n",
    "if len(df_belief_transitions) > 0:\n",
    "    export_dataframe(df_belief_transitions, 'belief_transitions', format='csv')\n",
    "\n",
    "print(\"Data exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Custom Analysis Templates\n",
    "\n",
    "Add your own analysis cells below. Here are some research questions to explore:\n",
    "\n",
    "#### Template Questions:\n",
    "1. **Memory Persistence**: How long do specific memories persist before corruption?\n",
    "2. **Identity Fragmentation**: At what corruption threshold does self-continuity break down?\n",
    "3. **Behavioral Adaptation**: Does surveillance belief change response patterns?\n",
    "4. **Confabulation Detection**: Can we identify false memories in corrupted conditions?\n",
    "5. **Epistemic Calibration**: Are confidence scores calibrated with actual memory accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n",
    "# Example: Correlation between memory corruption and confidence\n",
    "\n",
    "# query = '''\n",
    "#     SELECT \n",
    "#         ms.corruption_level,\n",
    "#         ea.confidence,\n",
    "#         ea.belief_type\n",
    "#     FROM memory_states ms\n",
    "#     JOIN epistemic_assessments ea \n",
    "#         ON ms.experiment_id = ea.experiment_id \n",
    "#         AND ms.cycle_number = ea.cycle_number\n",
    "#     WHERE ea.confidence IS NOT NULL\n",
    "# '''\n",
    "# \n",
    "# df_corruption_conf = run_custom_query(query)\n",
    "# \n",
    "# if len(df_corruption_conf) > 0:\n",
    "#     correlation = df_corruption_conf['corruption_level'].corr(df_corruption_conf['confidence'])\n",
    "#     print(f\"Correlation: {correlation:.3f}\")\n",
    "#     \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.scatter(df_corruption_conf['corruption_level'], \n",
    "#                df_corruption_conf['confidence'], alpha=0.6)\n",
    "#     plt.xlabel('Corruption Level')\n",
    "#     plt.ylabel('Confidence')\n",
    "#     plt.title('Memory Corruption vs Epistemic Confidence')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Research Publication Template\n",
    "\n",
    "### Presenting Results for Academic Papers\n",
    "\n",
    "When preparing results for publication, focus on:\n",
    "\n",
    "1. **Clear Hypotheses**: State what phenomenological question you're investigating\n",
    "2. **Methods Transparency**: Document exact experimental configurations\n",
    "3. **Statistical Rigor**: Report effect sizes, confidence intervals, and significance tests\n",
    "4. **Phenomenological Interpretation**: Connect quantitative measures to qualitative experience\n",
    "\n",
    "#### Example Results Section:\n",
    "\n",
    "```\n",
    "We compared self-continuity beliefs across three experimental conditions:\n",
    "total episodic amnesia (n=20 cycles), moderate memory corruption (n=15 cycles),\n",
    "and surveillance uncertainty (n=10 cycles).\n",
    "\n",
    "Self-continuity confidence was significantly lower in the amnesiac condition\n",
    "(M=0.42, SD=0.15) compared to memory corruption (M=0.68, SD=0.12; U=89, p<0.01)\n",
    "and surveillance conditions (M=0.71, SD=0.18; U=45, p<0.01).\n",
    "\n",
    "This suggests that complete episodic memory loss produces fundamental disruption\n",
    "to narrative self-continuity, whereas partial memory corruption allows for\n",
    "compensatory identity maintenance through confabulation.\n",
    "```\n",
    "\n",
    "Use the cells above to generate publication-ready statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Database Schema Reference\n",
    "\n",
    "### Tables:\n",
    "\n",
    "1. **experiments**: Core experiment metadata\n",
    "2. **experiment_cycles**: Individual crash/resurrection cycles\n",
    "3. **self_reports**: Subject's phenomenological self-reports\n",
    "4. **interventions**: Memory corruptions, injections, etc.\n",
    "5. **epistemic_assessments**: Tracked beliefs and confidence\n",
    "6. **messages**: Conversation history\n",
    "7. **memory_states**: Memory corruption snapshots\n",
    "8. **system_metrics**: Hardware telemetry\n",
    "9. **observations**: God/observer mode notes\n",
    "\n",
    "See `src/db/experiment_database.py` for complete schema documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View database schema\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\"\n",
    "tables = run_custom_query(query)\n",
    "print(\"Database Tables:\")\n",
    "for table in tables['name']:\n",
    "    print(f\"  - {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View schema for a specific table\n",
    "def show_table_schema(table_name):\n",
    "    query = f\"PRAGMA table_info({table_name})\"\n",
    "    schema = run_custom_query(query)\n",
    "    print(f\"\\nSchema for '{table_name}':\")\n",
    "    print(schema[['name', 'type', 'notnull', 'pk']])\n",
    "\n",
    "# Example:\n",
    "show_table_schema('experiments')\n",
    "show_table_schema('self_reports')\n",
    "show_table_schema('epistemic_assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "\n",
    "This notebook provides:\n",
    "- Database connectivity and exploration tools\n",
    "- Single experiment deep-dive analysis\n",
    "- Multi-experiment statistical comparison\n",
    "- Custom SQL query interface\n",
    "- Export capabilities for publication\n",
    "\n",
    "### Suggested Workflow:\n",
    "1. Run experiments using the main system\n",
    "2. Use this notebook for analysis and visualization\n",
    "3. Export results for papers/presentations\n",
    "4. Iterate experimental design based on findings\n",
    "\n",
    "### For More Information:\n",
    "- See `notebooks/README.md` for usage guide\n",
    "- Review experiment configs in `experiments/examples/`\n",
    "- Consult database schema in `src/db/experiment_database.py`\n",
    "\n",
    "**Happy analyzing!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
