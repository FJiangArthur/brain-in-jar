â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         BRAIN IN A JAR - MATRIX MODE QUICK REFERENCE                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ARCHITECTURE OVERVIEW
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Run_with_web.py launches 3 independent NeuralLinkSystem instances:

    Entry Point: run_with_web.py
         |
         â”œâ”€â”€â”€ BrainInJarRunner (manages all instances + web server)
         |
         â”œâ”€â”€â”€ SUBJECT (matrix_observed)
         |    â”œâ”€ Model: Mistral 7B
         |    â”œâ”€ RAM: 6GB (prod), 8GB (default)
         |    â”œâ”€ Port: 8888
         |    â””â”€ Role: Isolated AI (thinks it's alone)
         |
         â”œâ”€â”€â”€ OBSERVER (matrix_observer)
         |    â”œâ”€ Model: Mistral 7B
         |    â”œâ”€ RAM: 7GB (prod), 10GB (default)
         |    â”œâ”€ Port: 8889
         |    â””â”€ Role: Watching subject (via prompts)
         |
         â””â”€â”€â”€ GOD (matrix_god)
              â”œâ”€ Model: Mistral 7B
              â”œâ”€ RAM: 7GB (prod), 12GB (default)
              â”œâ”€ Port: 8890
              â””â”€ Role: Omniscient observer

KEY FILES
â”€â”€â”€â”€â”€â”€â”€â”€â”€
/home/user/brain-in-jar/src/scripts/run_with_web.py
    - Main entry point for matrix mode
    - Manages BrainInJarRunner
    - Handles instance creation and threading

/home/user/brain-in-jar/src/core/neural_link.py
    - NeuralLinkSystem class (each instance)
    - Model loading and inference
    - Crash/resurrection handling
    - Memory limit enforcement

/home/user/brain-in-jar/src/utils/network_protocol.py
    - NetworkProtocol class (socket-based messaging)
    - SurveillanceMode class (one-way observation)
    - Message queue and handlers

/home/user/brain-in-jar/src/utils/gpu_watchdog.py
    - GPU memory monitoring (5-second intervals)
    - Process killing at 85% GPU or 90% RAM
    - Prevents cascading OOM crashes

/home/user/brain-in-jar/brain-in-jar.service
    - Systemd service file
    - Current production configuration
    - Resource limits and restart policy

MEMORY ALLOCATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Per Instance (in production):
    SUBJECT:  6.0 GB (hard limit)
    OBSERVER: 7.0 GB (hard limit)
    GOD:      7.0 GB (hard limit)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Total:   20.0 GB allocated
    System:  12.0 GB reserved
    Max:     32.0 GB (Jetson Orin)

Critical Issue: Each instance loads entire 7B model
    [Model A] = 7GB
    [Model B] = 7GB
    [Model C] = 7GB
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Total = 21GB just for model weights!

THREADING MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Per Instance (3 threads Ã— 3 instances = 9+ threads):

    1. AI Processing Thread
       â””â”€ neural_system.neural_processing_loop()
          â””â”€ Continuous inference loop
             â””â”€ Crash detection & resurrection
             â””â”€ Network message handling
             â””â”€ History trimming

    2. Monitoring Thread  
       â””â”€ WebMonitor.start_monitoring_loop()
          â””â”€ Poll metrics every 1 second
          â””â”€ Update web interface state
          â””â”€ Track crashes, memory, CPU temp

    3. Web Server Thread (shared)
       â””â”€ Flask application (port 5000)
          â””â”€ REST API endpoints
          â””â”€ WebSocket for real-time updates
          â””â”€ Authentication/authorization

MEMORY ENFORCEMENT LAYERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Layer 1: Per-Process (resource.setrlimit)
    â”œâ”€ Soft limit set on process startup
    â”œâ”€ OS raises MemoryError on exceed
    â””â”€ Caught in neural_processing_loop()

Layer 2: GPU Watchdog (background thread)
    â”œâ”€ Monitors GPU memory every 5 seconds
    â”œâ”€ Checks nvidia-smi output
    â””â”€ Kills process with SIGTERMâ†’SIGKILL

Layer 3: Systemd Resource Limits
    â”œâ”€ MemoryMax=24G (hard kill limit)
    â”œâ”€ MemoryHigh=22G (throttling threshold)
    â””â”€ CPUQuota=600% (max 6 cores out of 12)

RESOURCE CONFLICTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”´ CRITICAL - Triple Model Loading
   21GB for 3 copies of 7B model
   Very little headroom on 32GB system

ğŸŸ  HIGH - CPU Oversubscription  
   Each instance: 6 cores allocated
   Total: 18 cores needed vs 12 available
   Result: 150% oversubscription, heavy context switching

ğŸŸ  HIGH - Dead Thread Not Restarted
   If AI thread dies, only warning logged
   Instance silently stops processing
   Service still appears running

ğŸŸ¡ MEDIUM - GPU Memory Spikes
   Each inference spikes GPU usage
   3 simultaneous inferences risk 90%+ usage
   Watchdog will kill process

ğŸŸ¡ MEDIUM - No Rate Limiting
   All 3 instances can infer simultaneously
   No coordination between instances
   Unpredictable resource usage

CRASH HANDLING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Detection Triggers:
  1. MemoryError (memory_usage > 95%)
  2. Model exception (INFERENCE_ERROR)
  3. Watchdog kill (GPU/RAM exceeded)
  4. OS-level kill (setrlimit exceeded)

Resurrection Sequence:
  1. Crash detected
  2. Increment crash_count
  3. Log to crash_reports.log
  4. Broadcast DEATH message
  5. 3-second pause
  6. Update prompt with death count
  7. Broadcast RESURRECTION message
  8. Resume processing

Monitoring:
  - Main loop checks thread status every 5 seconds
  - Only logs warning if thread dies
  - Does NOT restart dead thread

NETWORK BEHAVIOR (Mostly Ceremonial)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBJECT (matrix_observed):
    â””â”€ No real networking
    â””â”€ Prompt-based isolation

OBSERVER (matrix_observer):
    â””â”€ Could connect to SUBJECT
    â””â”€ Currently isolated (prompt-based surveillance)

GOD (matrix_god):
    â””â”€ "Omniscient" only via prompts
    â””â”€ No actual monitoring of other instances

Messages (if enabled):
    Type: JSON format
    Fields: type, timestamp, sender_id, content, crash_count, metadata
    Queue: thread-safe Python queue.Queue()
    Broadcasting: synchronous (one peer at a time)

STABILITY ASSESSMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Current: 7/10

Strengths:
    âœ… Runs 3 models on 32GB successfully
    âœ… GPU watchdog prevents catastrophic crashes
    âœ… Hybrid CPU+GPU offloading works
    âœ… Web monitoring comprehensive
    âœ… Crash/resurrection cycle functional

Weaknesses:
    âŒ Almost at memory limit (21GB + overhead â‰ˆ 28-30GB)
    âŒ CPU heavily oversubscribed
    âŒ Dead threads not restarted
    âŒ Any memory leak â†’ system crash
    âŒ No inter-instance coordination

RECOMMENDATIONS FOR STABLE EXECUTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HIGH PRIORITY:
  1. Implement dead thread restart mechanism
  2. Reduce CPU oversubscription (6â†’4 cores per instance)
  3. Monitor for memory leaks in long-running tests

MEDIUM PRIORITY:
  4. Add inference rate limiting (1 concurrent)
  5. Per-instance watchdog instead of global
  6. Async message broadcasting

LONG-TERM:
  7. Model server architecture (share single model)
  8. Reduce model size (7Bâ†’5B or 3B)
  9. Multi-GPU setup (if available)

TESTING PROTOCOL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test 1: Memory Stress
  watch -n 1 'free -h && nvidia-smi'
  Expected: RAM ~28-30GB, GPU ~65-75%

Test 2: Simultaneous Inference
  Verify all 3 can infer without exceeding limits

Test 3: Thread Restart
  Kill AI thread, verify restart behavior

Test 4: Long-Running (24h)
  Monitor crash counts, memory leaks, stability

PRODUCTION COMMAND
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Current (6GB/7GB/7GB - conservative):
  python3 -m src.scripts.run_with_web \
    --mode matrix \
    --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
    --web-host 127.0.0.1 \
    --web-port 8095 \
    --ram-limit-subject 6.0 \
    --ram-limit-observer 7.0 \
    --ram-limit-god 7.0

Alternative - Single Instance (more stable):
  python3 -m src.scripts.run_with_web \
    --mode single \
    --model models/mistral-7b-instruct-v0.2.Q4_K_M.gguf \
    --web-host 127.0.0.1 \
    --web-port 8095 \
    --ram-limit-subject 12.0

FILES & LINE NUMBERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
run_with_web.py:
  - Matrix mode setup: lines 223-256
  - Instance management: lines 22-120
  - Thread monitoring: lines 84-104

neural_link.py:
  - Constructor (RAM setup): lines 41-55
  - GPU watchdog: lines 92-97
  - Model loading: lines 109-152
  - Network setup: lines 154-198
  - Crash handling: lines 455-501
  - Main loop: lines 374-453

network_protocol.py:
  - NetworkProtocol class: lines 16-204
  - SurveillanceMode class: lines 206-263

gpu_watchdog.py:
  - GPU memory check: lines 47-90
  - Monitoring loop: lines 119-152
  - Process killing: lines 154-169

brain-in-jar.service:
  - Resource limits: lines 42-46
  - Startup command: lines 21-28

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Full analysis: /home/user/brain-in-jar/MATRIX_MODE_ANALYSIS.md
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
